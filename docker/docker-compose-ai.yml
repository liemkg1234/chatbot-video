version: "3.11"

services:
  vlm:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: vlm
    ports:
      - "7001:8000"
    volumes:
      - ./volumes/ai/:/models
    command: >
      -m /models/SmolVLM2-2.2B-Instruct.Q4_K_M.gguf
      -n 4096
      --host 0.0.0.0
      --port 8000
#      runtime: nvidia
#      deploy:
#        resources:
#          reservations:
#            devices:
#              - capabilities: [gpu]
    ipc: host
    healthcheck:
      test: curl -f http://localhost:8000/health || exit 1
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: always
    networks:
      - chatbot-video

#  em:
#    image: ollama/ollama:latest
#    container_name: em
#    volumes:
#      - ./llm/ollama/em.entrypoint.sh:/root/entrypoint.sh
#      - ./volumes/llm/models/em:/root/.ollama
#    environment:
#      - OLLAMA_HOST=http://0.0.0.0:8000
#      - OLLAMA_KEEP_ALIVE=-1
#      - OLLAMA_CONTEXT_LENGTH=8192
#    entrypoint: ["/bin/sh", "/root/entrypoint.sh"]
#    runtime: nvidia
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [gpu]
#    pull_policy: always
#    tty: true
#    restart: always
#    profiles:
#      - llm
#    networks:
#      - llm-lab

#  asr:
#    image: ghcr.io/ggml-org/whisper.cpp:main
#

# === Network ===
networks:
  chatbot-video:
    name: chatbot-video
    driver: bridge
